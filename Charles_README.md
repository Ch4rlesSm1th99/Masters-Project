# Project Aims and Data Notes

The overall aim of the project is to train a model for auroral event detection. The dataset used for training was 
collected in 1995 (although this may change if the model is too data hungry and data from other years are required) off 
of the SuperDarn radar network. 

The data is completely unlabelled other than a document containing twenty days noted by a PHD student 20 years
ago, where some event was detected at some point during that day. Due to the lack of labels two more general apporaches 
have been considered. The first is a more topic specific strategy which is an unsupervised strategy. The majority of 
literature on this topic goes down this route and typically involves some form of dynamic clustering algorithm to perform the event 
detection or classification. Some examples of these dynamic clustering algorithms which I have seen on various space plasma projects 
and githubs and might be worth reading up on are DMSCAN and GMM(GMMA). The other potentailly viable strategy is a self-supervised 
one which although has no literature specifically on this topic area of space plasma physics has had various uses for on projects 
using similar time series datasets. Initially I am going to give this self supervised strategy a go and see how it performs and if 
it performs poorly I have other options I can try.

The 1995 Data consists of around 2000 files of time series data, there are three measurable features which may indicate the events 
we are interested in detecting and these three features are **power** , **velocity** and **spectral width**. 


# Unsupervised strategy .... (to be continued)
May explore in the future ....

# Self-Supervised Strategy


## 1. Modality Treatment
I think it makes sense to give the model access to as much information as possible so for training so it has more context to make 
decisions. Therefore I think it is a good idea to treat each one of our three detectable features as seperate modilities and train 
the model on all three. The reason being that if we treat them as seperate modalities rather than as feature (would be same datatype)
it will give us more flexibility on how to combine these datatypes along the pipeline.

With this being said the time series data will need to be chopped up and fed into the model as samples rather than treating this as a
regression problem. Now the segments which we decide to chop the time series data up into I think ideally needs to be safely longer than
the average time of one of these events and the chopping up of the data may need to be fine tuned to assist learning.

## 2. Data Augmentation
Augmented views of the original samples will be generated by either injecting noise or introducing shifts in the data hopefully to present 
alternate views of the same piece of data. The goal being to slightly alter local data trends but without greatly affecting the global trends. 
With this I am planning to visualise the original data and augmentations to check by eye across lots of sample for consistency to check that
we aren't training on botched data.

Data augmentations are likely to be done in the dataloaders themselves rather than generating a new dataset or having a script to do it elsewhere 
as I think that makes most sense.

## 3. Feature Extraction
Pass all three modalities throgu a 1D CNN where either the weights are learnt seperately or combined weights are learnt I am unsure at the moment.
Pretrained or fine tuning CNN's I don't belive will be a good idea as the dataset in this problem is very unique to this scenario and will probablly 
work best if the CNN guesses from scratch the weights and then lets the optimiser work its magic.

## 4. Modality Fusion

After feature extraction the features for each modality will be concatenated on top of eachother to form a feature space which represents all data types.
It might be worth maybe experimenting which data types are the biggest contributors to the detected and combing in some type of hierrchical fusion strategy.

## 5. Self-Supervised Strategy
We use an alignement strategy most likely contrastive loss function (SimCLR maybe) or triplet loss function to learn to pull together paired augmentations
of the original data and push away contrasting pairs of data. Essentially the paired augmentations act as generate labels for the dataset so its eseentially
trying to cheese a supervised strategy on the unlablled dataset.

Three problems that will be needed to solve is the accuracy metric for alot of these are batch accuracy metrics rather than global accuracy metrics which may
 give a false impression of how the model will perfrom on the test data without augmentations. 


This will need significantly large batchsizes at testing stage and so we may run into a shortage of RAM for loading at testing potentailly. 

Another problem is that the model wont necesarily be trained to class events more select samples that look similar to a given sample from across a particular batch. 
This is something I will address in the next section. 

**technically at this stage we will have a model that can single out specific events although not specifically classify them. This could be fixed but would require 
generating a pseudo-labelled dataset ** 

## 6. Pseudo Labelling (Optional if persueing this model further)

In the case of event detection we could use the days mentioned as detected events from the PHD students document and use those as our classes to try and compile a 
set of very likely event detection and use those to train a simpler model potentially?


Sorry this read me is so casual if you have read this far I got tired!



